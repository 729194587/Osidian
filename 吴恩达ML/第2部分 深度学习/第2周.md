
#### 激活函数
(1)**ReLU（Rectified Linear Unit，线性整流单元）是深度学习中最常用的激活函数之一,定义如下:**
$$\mathrm{ReLU}(x)=
\begin{cases}
x, & x>0, \\
0, & x\le 0.
\end{cases}
$$
或者是：
$$\mathrm{ReLU}(x)=\max(0, x)
$$
(2)线性激活函数：
$$f(x) = x
$$
(3)sigmoid函数

如何选择**输入层激活函数？
- 二分类问题，用sigmoid函数。
- 如果是预测股票之类的，可能下降可能上升，用线性激活函数。
- 如果是预测房价之类的非负数，使用ReLU函数。（房价永远不会跌嘛）

如何选择**隐藏层激活函数？**
现在最常用的函数是ReLU函数。

**比较sigmoid函数和ReLU函数：**
- ReLU函数计算速度快。
- ReLU函数只有在y轴左边才会平坦，而sigmoid函数在两侧都会平坦，这会导致梯度下降速度变慢。

**为什么需要激活函数呢？**
不用的话，模型退化成了线性回归模型。

### 多分类

#### softmax算法

Softmax 是多分类任务中最常用的输出层激活函数，它可以将一个 **向量** 转换成一个 **概率分布**，每个元素都介于 0 与 1 之间，且总和为 1。

$$\mathrm{Softmax}(z_i)=\frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}
$$
#### 损失函数

Softmax 的损失函数（交叉熵）是用“真实标签 y 的值”所对应的预测值来计算的。
$$L =
\begin{cases}
-\log(\hat{p}_1), & y = 1,\\[4pt]
-\log(\hat{p}_2), & y = 2,\\[4pt]
\vdots & \vdots\\[4pt]
-\log(\hat{p}_K), & y = K.
\end{cases}
$$

很容易理解，如果真实值是y，那么对该项的预测概率越接近于1，模型效果越好。而越接近于1时，上面的损失函数就越从正向趋于0。

tensorflow中把这个称为稀疏交叉熵损失函数，稀疏指的是只能输出k个类别中的一个。

代码是这样的：
```python
model.add(Dense(10, activation='softmax'))
loss = SparseCategoricalCrossentropy()
```
一种优化方式是这样：
```python
model.add(Dense(10, activation='softmax'))
loss = SparseCategoricalCrossentropy(from_logits=True)
```
通过把传入损失函数中的变量变成原始的计算公式，而不是中间变量，给予TensorFlow更多的优化空间，让计算结果更精确。
$$\left(1 + \frac{1}{10000}\right) - \left(1 - \frac{1}{10000}\right) \neq \frac{2}{10000}
$$
如果你直接传入两个括号内的计算结果，可能会有精度损失。
而你传入等号左边的式子的话，tensorflow会进行排列计算，使结果精确。

#### Adam算法（Adaptive Moment Estimation）

Adam 是目前最常用的优化算法之一（几乎所有深度学习模型默认优化器都是 Adam）。

Adam算法给每一个参数都设有学习率。

当参数一直朝着大致相同的方向前进，就增大学习率；
如果参数来回震荡，就减小学习率。

```python
#简单的模型示例

#model
model = tf.keras.Sequential([
    tf.keras.layers.Dense(25, activation='relu'),
    tf.keras.layers.Dense(15, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
])
#compile
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics=['accuracy']
)
#fit
model.fit(x_train, y_train, epochs=20, batch_size=32)

```

### 连接层类型

之前所用的连接层都是全连接层，每个神经元的输入都是上一层的所有输出。

#### 卷积层

每个神经元仅查看部分的前一层的输出。
- 加快计算速度。
- 需要的训练数据更少。
在图像识别上，就表现为：一个神经元查看部分图像区域。

今天很多神经网络的新研究都源自于研究者为神经网络发明新的类型的层，并将不同类型的层组合在一起。

#### 反向传播算法

反向传播（Backpropagation, BP）是一种 **利用链式法则计算神经网络中所有参数梯度** 的算法。

$$\frac{\partial L}{\partial W}
=
\frac{\partial L}{\partial a}
\cdot
\frac{\partial a}{\partial z}
\cdot
\frac{\partial z}{\partial W}
$$
利用链式求导法则从后向前可以很方便求最终结果对各个参数的梯度，每个中间变量只需要计算一次。

本质上是动态规划。
