### 多特征

有时候，输入x可能会有很多个参数。
那么预测函数f就变成了：
$$
\hat{y} = w_1 x_1 + w_2 x_2 + \cdots + w_n x_n + b
$$
用向量化表示为：
$$
\hat{y} = \mathbf{w}^T \mathbf{x} + b
$$
其中
$$
\mathbf{x} = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix}, \quad 
\mathbf{w} = \begin{bmatrix} w_1 \\ w_2 \\ \vdots \\ w_n \end{bmatrix}
$$


### 向量化
向量化是指**用向量/矩阵运算替代显式的 for 循环**来执行数学计算。

单特征时：
```c
y_pred = np.zeros(m)
for i in range(m):
    y_pred[i] = w * x[i] + b
```
每次迭代都要写循环，运行很慢。
向量化后
```python
y_pred = w * x + b
```
多特征情况下，
```python
y_pred = np.dot(X, w) + b
```
梯度下降更新的公式是：
$$
\theta_j := \theta_j - \alpha \frac{1}{m}\sum_{i=1}^{m} (h_{\theta}(x^{(i)}) - y^{(i)}) x_j^{(i)}
$$
- 对每个 j 从 0 到 n，都要更新，j=0,θ是b。
- **必须使用临时变量** 或一起更新（也就是向量化）。

##### 正规方程

正规方程可以直接算出参数，不需要梯度迭代。
但是时间开销大，参数量大的话无法使用。O(N^3)

### 特征缩放

一些情况下，不同输入的x的数值可能会相差好几个数量级，像预测房价时房子的面积和卧室数量这两个输入，这样的话，会导致梯度下降十分缓慢，可以对特征进行缩放来统一尺度。

#### 为什么需要特征缩放？

- 避免某些特征支配模型
- 可以选用更大的学习率，让梯度下降更稳定、更快收敛
- 避免数值不稳定问题，计算协方差矩阵、距离时可能出现浮点溢出或极高的方差。


**常见特征缩放方法**：
- 标准化
$$
x' = \frac{x - \mu}{\sigma}
$$

- 最小-最大缩放
$$
x' = \frac{x - \min(x)}{\max(x) - \min(x)}
$$

如果要缩放到`[a,b]` :
$$
x' = a + \frac{(x - \min(x))(b - a)}{\max(x) - \min(x)}
$$

代价函数随着迭代次数变化的曲线称为**学习曲线**。
理想情况下，代价函数应该随着迭代次数增加而逐渐降低。

如果代价函数增加，那么说明学习率选择的大了，或者代码有问题。

对于不同应用，代价函数趋于平稳的迭代次数有很大的不同。有的可能几十次，有的可能数万次。

可以根据学习曲线来判断收敛，也可以计算代价函数差值来判断收敛。

### 特征工程

模型通常有很多特征，需要寻找一些能更好预测的特征。
有时还需要寻找一些新特征，例如，根据房子的长和宽来计算出占地面积。



#### 多项式回归

把特征工程和多元线性回归结合，我们可以得到多项式回归。

k阶多项式特征：
$$
\phi(x) = \left[\, 1,\ x,\ x^{2},\ \ldots,\ x^{k} \right]
$$
那么模型表达式为：
$$hat{y}(x) = w_0 + w_1 x + w_2 x^2 + \cdots + w_k x^k
$$
还可以选择根号x作为一个特征。

此外，特征缩放和特征选择变得十分重要。

特征的取值会变得很大，需要适当的特征缩放。
特征选择更不用说。


