
机器学习的两种主要类型：**有监督学习和无监督学习**

重点：如何设计和构建严格的机器学习系统？

### 监督学习(Supervised Learning)

通过给定的带标签训练数据 `(xi​,yi​)`，学习一个从输入空间到输出空间的映射函数 f:X→Y，使得模型对未见过的新样本也能给出准确的预测。


回归任务的目标是学习一个从输入 x到**连续值输出** y 的映射函数。也就是说，标签是实数或数值域上的量。例如预测房价。

分类任务的目标是学习一个从输入 x 到**离散类别标签** y 的映射函数。标签以有限个类别表示。
例如图像分类，医学诊断。

分类的输出一般比回归少。

### 无监督学习(Unsupervised Learning)

无监督学习是一类在没有标签的数据上进行学习的算法，它通过分析输入数据的结构、分布或内部关系，自动发现数据中的模式或潜在结构。

输入x，没有对应的y。目的就是为了总结数据的规律。

① 发现数据的潜在结构（结构学习）。
② 找到数据的分布模式。
**③ 降维（Dimension Reduction）用较少的维度表示原始数据，同时尽量保留信息。**
把大数据集变成小数据集。
④ 数据压缩、特征抽取。

**聚类**是一类典型的无监督学习方法，其目标是在没有标签的数据中，根据样本之间的相似性，将数据自动划分成若干组（簇）。同一簇内的样本彼此更相似，不同簇之间的样本差异更大。

##### **K-Means（最经典）**
- 通过迭代更新簇中心，将数据点分为 K 个簇
- 基于“最小化簇内平方误差”
特点：简单、高效，但要求事先给定 K。


**训练集样本通常用上标（括号里的索引）来表示。**(x(i),y(i))。

先将训练集输入到监督学习算法中，算法会生成一些函数f，称为假设。

用f去预测输出y。
线性回归：`f(x)=wx+b`;

#### 代价函数(cost function)

用来衡量 **模型预测结果与真实值之间误差大小** 的一个函数。
对于监督学习，代价函数通常是这样的：

		$J(\theta) = \frac{1}{m} \sum_{i=1}^m L(\hat{y}^{(i)}, y^{(i)})$

- mmm：训练样本数
- $\hat{y}^{(i)}$：模型第 i 个样本的预测值
- $y^{(i)}$：真实值
- L(⋅)：单个样本的损失，比如平方误差、交叉熵等
- θ：模型参数（比如线性回归里的 `w、b`）

训练就是找一组参数 θ，让 J(θ) 最小。

```python
# UNQ_C1
# GRADED FUNCTION: compute_cost

def compute_cost(x, y, w, b): 
    """
    Computes the cost function for linear regression.
    计算线性回归的代价函数。
    Args:
        x (ndarray): Shape (m,) Input to the model (Population of cities) 
        y (ndarray): Shape (m,) Label (Actual profits for the cities)
        w, b (scalar): Parameters of the model
    
    Returns
        total_cost (float): The cost of using w,b as the parameters for linear regression
               to fit the data points in x and y
    """
    # number of training examples
    #训练样本的数量
    m = x.shape[0] 
    
    # You need to return this variable correctly
    total_cost = 0
    
    ### START CODE HERE ###  
    # The prediction of the model for that example
    y_pred = np.zeros(m)
    for i in range(m):
        y_pred[i] = x[i] * w + b
    # The cost 是一个numpy数组
    cost = (y_pred - y)**2
    for i in range(m):
        total_cost += cost[i]
        
    total_cost = total_cost/(2*m)
    ### END CODE HERE ### 
    return total_cost
```


```python
#向量化计算cost
def computeCost(X, y, theta):
    """
    Compute cost for linear regression. Computes the cost of using theta as the
    parameter for linear regression to fit the data points in X and y.
    
    Parameters
    ----------
    X : array_like
        The input dataset of shape (m x n+1), where m is the number of examples,
        and n is the number of features. We assume a vector of one's already 
        appended to the features so we have n+1 columns.
    
    y : array_like
        The values of the function at each data point. This is a vector of
        shape (m, ).
    
    theta : array_like
        The parameters for the regression function. This is a vector of 
        shape (n+1, ).
    
    Returns
    -------
    J : float
        The value of the regression cost function.
    
    Instructions
    ------------
    Compute the cost of a particular choice of theta. 
    You should set J to the cost.
    """
    
    # initialize some useful values
    m = y.size  # number of training examples
    
    # You need to return the following variables correctly
    J = 0
    #向量化版本
    # predictions = X.dot(theta)  # shape (m,)
    # errors = predictions - y    # shape (m,)
    # J = (1/(2*m)) * np.dot(errors, errors)

    # ====================== YOUR CODE HERE =====================
    for i in range(m):
        x=X[i,:]
        cost=x.dot(theta)-y[i]#减去第i个样本的真实值
        J+=cost**2
    J=J/(2*m)
    # ===========================================================
    return J
```
####  梯度下降(Gradient descent)

沿着代价函数下降最快的方向（负梯度方向），一步一步走，直到找到最低点（最优参数）。

- 设置参数的初始值。
- 稍微改变参数，减少代价函数值。
- 知道代价函数接近或成为最小值。
- 有的代价函数可能含有多个最小值。(局部最值)

$$
\theta = \theta - \alpha \cdot \nabla_\theta J(\theta)
$$
或者是：
$$
\theta := \theta - \alpha \frac{\partial J(\theta)}{\partial \theta}
$$
- $\frac{\partial J}{\partial \theta}​$：梯度（斜率）
- α：学习率（learning rate），控制每一步走多大
- “:=” 表示更新参数。

如果有两个参数，对两个参数进行更新：
```c
theta0 = theta0 - alpha * dJ_dtheta0;
theta1 = theta1 - alpha * dJ_dtheta1;  
```
**这样更新参数是错误的！
这里的 dJ_dtheta1 已经使用了更新后的 theta0！**
两个参数的导数应该基于完全相同的旧参数计算。
正确版：
```c
double new_theta0 = theta0 - alpha * dJ_dtheta0;
double new_theta1 = theta1 - alpha * dJ_dtheta1;

theta0 = new_theta0;
theta1 = new_theta1;
```

```python
def compute_gradient(x, y, w, b): 
    """
    Computes the gradient for linear regression 
    ----
    计算线性回归的梯度

    Args:
      x (ndarray): Shape (m,) Input to the model (Population of cities) 
      x（ndarray）：形状为 (m,) 的模型输入（城市人口）

      y (ndarray): Shape (m,) Label (Actual profits for the cities)
      y（ndarray）：形状为 (m,) 的标签（城市的实际利润）

      w, b (scalar): Parameters of the model  
      w, b（标量）：模型参数

    Returns
      dj_dw (scalar): The gradient of the cost w.r.t. the parameters w
      dj_dw（标量）：代价函数对参数 w 的梯度

      dj_db (scalar): The gradient of the cost w.r.t. the parameter b     
      dj_db（标量）：代价函数对参数 b 的梯度
     """
    
    # Number of training examples
    # 训练样本数量
    m = x.shape[0]
    
    # You need to return the following variables correctly
    # 你需要正确返回以下变量
    dj_dw = 0
    dj_db = 0
    ### START CODE HERE ### 
    # START CODE HERE（在这里开始写代码）  

    y_pred = np.zeros(m)
    # 创建一个长度为 m 的数组，用来存放模型的预测值

    for i in range(m):
        y_pred[i] = x[i] * w + b
        # 对每个样本计算预测值：w*x + b
        
    dj_db = np.sum(y_pred - y)/m
    # 计算 b 的梯度：所有误差之和 / m

    dj_dw = np.sum((y_pred - y) * x)/m
    # 计算 w 的梯度：误差 * x 的和 / m
    
    ### END CODE HERE ###
    # END CODE HERE（代码结束）
        
    return dj_dw, dj_db
    # 返回两个梯度
```


**凸函数(convex function)**
凸函数在全局上只有一个最优值，不会陷入局部最优。

### 批次梯度下降(batch gradient descent)

批处理是一次性运行所有示例。

```python
def gradient_descent(x, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters): 
    """
    批量梯度下降函数，用于学习模型参数 theta。
    通过进行 num_iters 次梯度更新，并使用学习率 alpha 更新参数。

    参数:
      x : ndarray, 形状 (m,) 输入特征
      y : ndarray, 形状 (m,) 标签
      w_in, b_in : 标量，模型初始参数
      cost_function : 计算代价的函数
      gradient_function : 计算梯度的函数
      alpha : float, 学习率
      num_iters : int, 迭代次数

    返回值:
      w : ndarray, 迭代结束后的参数 w
      b : 标量, 迭代结束后的参数 b
      J_history : list, 每次迭代的代价值
      w_history : list, 每次迭代的参数 w，用于绘图
    """
    
    # 训练样本数量
    m = len(x)
    
    # 用于记录每次迭代的代价和 w 值，主要用于绘图
    J_history = []
    w_history = []

    # 深拷贝 w_in 避免在函数内部修改外部变量
    w = copy.deepcopy(w_in)  
    b = b_in
    
    for i in range(num_iters):

        # 计算梯度
        dj_dw, dj_db = gradient_function(x, y, w, b )  

        # 使用梯度下降公式更新参数
        w = w - alpha * dj_dw               
        b = b - alpha * dj_db               

        # 保存每次迭代的代价（最多保存前 100000 次，防止内存占用过大）
        if i < 100000:
            cost = cost_function(x, y, w, b)
            J_history.append(cost)

        # 每运行总步数的 1/10 打印一次代价；如果总迭代次数 <10，则每次都打印
        if i % math.ceil(num_iters / 10) == 0:
            w_history.append(w)
            print(f"Iteration {i:4}: Cost {float(J_history[-1]):8.2f}   ")
        
    # 返回最终的 w, b 以及代价和 w 的历史记录
    return w, b, J_history, w_history

```
运行结果：![[梯度下降运行结果.png]]





