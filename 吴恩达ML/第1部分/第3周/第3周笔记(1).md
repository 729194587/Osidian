
# 分类

当涉及到分类的时候，通常要求模型输出的十分有限。
像判断一封邮件是否是垃圾邮件，判断一个肿瘤良性还是恶性。

线性回归模型在预测分类时效果不好。可能会因为一些极端的数据导致预测线偏移，从而产生错误的判断。

### 逻辑回归

#### sigmoid（Logistic）函数

```python
def sigmoid(z):
    """
    Compute the sigmoid of z

    Args:
        z (ndarray): A scalar, numpy array of any size.

    Returns:
        g (ndarray): sigmoid(z), with the same shape as z
         
    """
    g = 1/(1+np.exp(-z))
   
    return g
```
$$
\sigma(x) = \frac{1}{1 + e^{-x}}

$$
逻辑回归先用线性回归计算一个分数，再通过 sigmoid 函数映射到概率区间 `[0,1]`
$$z = w_0 + w_1 x_1 + \dots + w_n x_n
$$$$
\hat{y} = \sigma(z) = \frac{1}{1 + e^{-z}}
$$
逻辑回归不止能用线性回归来计算sigmoid函数的参数。
还可以用一些更复杂的多项式回归。
$$
z = w_0 + w_1 x + w_2 y + w_3 x^2 +w_4 x^3
$$

y的取值范围是0到1，通常采用一个阈值来输出结果，例如0.5，当y大于0.5的时候，输出1，dangz小于0.5的时候，输出0。

那么当y等于0.5时输出什么？

我注意到模型的各个数据的分布到y=0.5的距离是不同的，也许可以把y=0.5归到离它最近的那些点的输出里？

y的输出对应的各个输入x的取值范围可以通过解方程组得到。
y<0.5对应<0，y>0.5对应z＞0。

#### 代价函数

线性回归的平方误差不再适用于逻辑回归，需要其他的损失函数来训练模型。
真实的数据要么是1要么是0，而且变化是突变的，很容易出现多个局部最小值。

逻辑回归采用 **交叉熵损失**。
$$L(y,\hat{y}) = 
\begin{cases}
-\log(\hat{y}), & y = 1, \\
-\log(1 - \hat{y}), & y = 0.
\end{cases}
$$
- 当真实值是1时，单个预测值的损失函数是-logy。
如果预测值距离1越近，它就越小；如果预测值距离0越近，它就越大。
- 当真实值是0时，单个预测值的损失函数是-log(1-y)。
预测值距离0越近，损失函数越小；距离1越近，损失越大。
- f(x)预测值距离真实值越远，损失越高。

也可以把两个公式合起来。
$$
L(y,\hat{y}) = -\left[ y \log(\hat{y}) + (1 - y)\log(1 - \hat{y}) \right]
$$

**似乎是用最大似然函数得到的**

**最大似然(maximum likelihood)**
对伯努利分布取对数似然函数。损失函数是其负数。
$$L(w) = \prod_{i=1}^{m} P\big(y^{(i)} \mid x^{(i)}, w\big)
$$
$$P(y^{(i)} \mid x^{(i)}, w)
= \hat{y}^{\,y^{(i)}} (1 - \hat{y})^{1 - y^{(i)}}
$$

考研结束未到一年，我就已经快忘记了似然函数这些玩意。。

就是把各个实验数据的概率乘在一块，求这玩意的极值。
然后得到对应的参数，意义就是：
**在参数为该值的时候，我们观测到这组数据的概率最高。这样看，最大似然估计作为损失函数来求概率确实很合理。**
取对数方便计算。

而又因为我们在机器学习中习惯“**最小化损失函数**”，而对数似然是要“最大化”的，因此取负号把最大化问题变成最小化问题。

所以，损失函数是负对数似然函数。


```python
def compute_cost_logistic(X, y, w, b):
    """
    Computes cost

    Args:
      X (ndarray (m,n)): Data, m examples with n features
      y (ndarray (m,)) : target values
      w (ndarray (n,)) : model parameters  
      b (scalar)       : model parameter
      
    Returns:
      cost (scalar): cost
    """
    m = X.shape[0]
    cost = 0.0
    for i in range(m):
        z_i = np.dot(X[i],w) + b
        f_wb_i = sigmoid(z_i)
        cost +=  -y[i]*np.log(f_wb_i) - (1-y[i])*np.log(1-f_wb_i)
             
    cost = cost / m
    return cost
```
#### 梯度下降

首先，考虑z由x的一次幂线性组合而成。

##### 模型定义：
$$\hat{y}^{(i)} = \sigma(z^{(i)}) = \frac{1}{1 + e^{-z^{(i)}}}, \quad z^{(i)} = w^T x^{(i)} + b
$$
单个样本的损失函数：
$$\ell^{(i)}(w) = -\Big(y^{(i)}\log \hat{y}^{(i)} + (1-y^{(i)})\log(1 - \hat{y}^{(i)})\Big)
$$
求损失函数对于w的偏导数，利用链式法则：
$$\frac{\partial \ell^{(i)}}{\partial w_j} 
= \frac{\partial \ell^{(i)}}{\partial \hat{y}^{(i)}} \cdot 
\frac{\partial \hat{y}^{(i)}}{\partial z^{(i)}} \cdot 
\frac{\partial z^{(i)}}{\partial w_j}
$$
$$
\frac{\partial \ell^{(i)}}{\partial \hat{y}^{(i)}} = -\frac{y^{(i)}}{\hat{y}^{(i)}} + \frac{1 - y^{(i)}}{1 - \hat{y}^{(i)}}
$$
$$\frac{\partial \hat{y}^{(i)}}{\partial z^{(i)}} = \hat{y}^{(i)} (1 - \hat{y}^{(i)})
$$
$$\frac{\partial z^{(i)}}{\partial w_j} = x_j^{(i)}
$$
化简可得：
$$\frac{\partial \ell^{(i)}}{\partial w_j} 
= \left(-\frac{y^{(i)}}{\hat{y}^{(i)}} + \frac{1 - y^{(i)}}{1 - \hat{y}^{(i)}}\right) 
\cdot \hat{y}^{(i)} (1 - \hat{y}^{(i)}) \cdot x_j^{(i)}
= (\hat{y}^{(i)} - y^{(i)}) x_j^{(i)}
$$
最后可得：
$$\nabla_w J(w) = \frac{1}{m} \sum_{i=1}^m (\hat{y}^{(i)} - y^{(i)}) x^{(i)}
$$
b的也很快能得出：
$$\nabla_b J(w) = \frac{1}{m} \sum_{i=1}^m (\hat{y}^{(i)} - y^{(i)})
$$
```python
def compute_gradient_logistic(X, y, w, b): 
    """
    Computes the gradient for linear regression 
 
    Args:
      X (ndarray (m,n): Data, m examples with n features
      y (ndarray (m,)): target values
      w (ndarray (n,)): model parameters  
      b (scalar)      : model parameter
    Returns
      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. 
      dj_db (scalar)      : The gradient of the cost w.r.t. the parameter b. 
    """
    m,n = X.shape
    dj_dw = np.zeros((n,))                           #(n,)
    dj_db = 0.

    for i in range(m):
        f_wb_i = sigmoid(np.dot(X[i],w) + b)          #(n,)(n,)=scalar
        err_i  = f_wb_i  - y[i]                       #scalar
        for j in range(n):
            dj_dw[j] = dj_dw[j] + err_i * X[i,j]      #scalar
        dj_db = dj_db + err_i
    dj_dw = dj_dw/m                                   #(n,)
    dj_db = dj_db/m                                   #scalar
        
    return dj_db, dj_dw 
```

##### 批量梯度下降(Batch Gradient Descent)
```python
# An array to store cost J and w's at each iteration primarily for graphing later
# 用于存储每次迭代的损失值 J 和权重 w，主要是为了后续绘图观察
J_history = []

w = copy.deepcopy(w_in)  #avoid modifying global w within function
# 深拷贝初始权重 w_in，避免修改函数外部的全局 w

b = b_in  # 初始化偏置 b

for i in range(num_iters):
    # Calculate the gradient and update the parameters
    # 计算梯度并更新参数
    dj_db, dj_dw = compute_gradient_logistic(X, y, w, b)   

    # Update Parameters using w, b, alpha and gradient
    # 使用权重 w、偏置 b、学习率 alpha 和梯度更新参数
    w = w - alpha * dj_dw               
    b = b - alpha * dj_db               
  
    # Save cost J at each iteration
    # 保存每次迭代的损失值
    if i < 100000:      # prevent resource exhaustion 
        # 避免资源耗尽，限制保存迭代次数
        J_history.append(compute_cost_logistic(X, y, w, b))

    # Print cost every at intervals 10 times or as many iterations if < 10
    # 每隔迭代总次数的 1/10 打印一次损失，或者如果迭代次数少于 10，则打印每次
    if i % math.ceil(num_iters / 10) == 0:
        print(f"Iteration {i:4d}: Cost {J_history[-1]}   ")

# return final w,b and J history for graphing
# 返回最终的权重 w、偏置 b，以及损失历史 J_history（用于绘图观察）
return w, b, J_history
```



